# 特征选择

## 更高阶的预处理和特征选择

特征选择指的是辨别和选择最相关、最有信息量的特征，以提升模型性能和减少计算复杂度的过程。在机器学习中，特征选择是一个关键步骤，因为它可以帮助我们去除冗余或无关的特征，从而提高模型的泛化能力。

![1761035668126](lecture9.assets/1761035668126.png)

优点

- 加速训练
  - 模型不需要区学习不相关的特征
- 减少过拟合
  - 避免模型从不相关的数据中学习到噪声
- 提高可解释性
  - 更少的特征使得模型更容易理解和解释

但是，特征选择也存在隐患

- 数据泄露 (Data Leakage)
  - 在特征选择过程中使用了测试集的信息，导致模型在测试集上表现异常好，但在实际应用中表现不佳。
- 多重共线性 (Multicollinearity)
  - 当两个或多个特征高度相关时，可能会导致模型不稳定，影响模型的性能和解释性。
- 目标泄漏 (Target Leakage)
  - 训练特征中包含了与目标变量直接相关的信息，导致模型在训练时表现过于理想，但在实际应用中效果不佳。
- 忽略特征关系
  - 某些特征可能单独看起来不重要，但与其他特征结合时可能具有重要意义。简单的特征选择方法可能会忽略这些复杂的关系。

常用的特征选择方法有

- 过滤法 (Filter Methods)
  - 基于统计指标（如相关系数、卡方检验等）来评估每个特征与目标变量的关系，选择得分最高的特征。
  - 快速且易于实现，但可能忽略特征之间的相互关系。
- 包装法 (Wrapper Methods)
  - 使用特定的机器学习算法作为评估标准，通过递归特征消除（RFE）等方法，选择最优特征子集。
  - 能够考虑特征之间的相互关系，但计算成本较高。
- 嵌入法 (Embedded Methods)
  - 在模型训练过程中进行特征选择，如 Lasso 回归中的 L1 正则化，可以自动选择重要特征。
  - 综合了过滤法和包装法的优点，效率较高。

## 过滤法 (Filter Methods)

常用的过滤法有

- 方差阈值法 (Variance Threshold)
  - 移除方差低于阈值的特征，他们被视为常数特征或近似常数特征
  - 数据首先需要标准化
- 单变量统计检验
  - 包括：卡方检验 (Chi-Squared Test)，F 检验 (F-test)
  - 可以做到模型无关
  - 易于计算，适用于高阶数据的初步过滤
  - 不能捕捉特征之间的相互作用，可能会丢失一些有用的信息
- 相互信息法 (Mutual Information)

### 卡方检验

卡方检验是一种用于评估两个分类变量之间关联强度的统计方法。在特征选择中，卡方检验可以用来评估每个分类特征与目标变量之间的关系，从而选择最相关的特征。

1. 确定零假设和备择假设

   - 零假设 (H0)：特征与目标变量之间没有关联
   - 备择假设 (H1)：特征与目标变量之间存在关联

2. 计算卡方统计量
   $$ \chi^2 = \sum \frac{(O*{ij} - E*{ij})^2}{E\_{ij}} $$

   - 其中，$O_{ij}$ 是观察频率，$E_{ij}$ 是期望频率

3. 更高的卡方值表示特征与目标变量之间的关联更强，根据给定的显著性水平 (如 0.05) 来决定是否拒绝零假设

在卡方检验中实现特征选择方法的步骤是：

- 连续变量分箱 (Binning)
  - 将连续变量转换为分类变量
  - 每个区间内的期望频率应足够大（通常至少为 5）
- 测试统计量计算
  - 计算每个特征与目标变量之间的卡方统计量
- 根据预设条件进行特征选择
  - 可以对测试统计量排序，选择前 k 个特征或根据显著性水平选择特征
  - 可以设置显著性水平 (如 0.05) 来决定是否保留特征

### F 检验

F 检验是一种用于比较多个组之间均值差异的统计方法。在特征选择中，F 检验可以用来评估每个数值特征与分类目标变量之间的关系，从而选择最相关的特征。

1. 确定零假设和备择假设

   - 零假设 (H0)：不同类别的均值相等
   - 备择假设 (H1)：至少有一个类别的均值不相等

2. 计算 F 统计量

   $$ F = \frac{MSB}{MSW} = \frac{SSB/(k-1)}{SSW/(n-k)} $$

   - 其中，$MSB$ 是组间均方，$MSW$ 是组内均方
   - $MSB = \frac{SSB}{dfB} = \frac{SSB}{k-1}$，$MSW = \frac{SSW}{dfW} = \frac{SSW}{N-k}$
     - $k$ 是类别数，$N$ 是总样本数
   - $SSB = \sum n_i(\bar{X}_i - \bar{X})^2$，$SSW = \sum \sum (X_{ij} - \bar{X}_i)^2$
     - 其中，$n_i$ 是第 $i$ 类的样本数，$\bar{X}_i$ 是第 $i$ 类的均值，$\bar{X}$ 是总体均值

3. 根据给定的显著性水平确定检验的临界值，比较计算得到的 F 统计量与临界值，决定是否拒绝零假设

实现 F 检验方法的具体步骤是

- 连续数据分箱
  - 将连续变量转换为分类变量
  - 每个箱子的预期频率应足够大（通常至少为 5）
- 检验统计量计算
  - 计算每个特征与目标变量之间的 F 统计量
- 特征选择
  - 对检验统计量进行排序，选择前 k 个特征或根据显著性水平选择特征
  - 可以设置显著性水平 (如 0.05) 来决定是否保留特征

### 相互信息法

相互信息 (Mutual Information) 是一种衡量两个随机变量之间相互依赖关系的度量。在特征选择中，相互信息可以用来评估每个特征与目标变量之间的信息共享程度，从而选择最相关的特征。

$$
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \left( \frac{p(x, y)}{p(x)p(y)} \right)
$$

- 其中，$I(X; Y)$ 是特征 $X$ 与目标变量 $Y$ 之间的相互信息
- $p(x, y)$ 是 $X$ 和 $Y$ 的联合概率分布
- $p(x)$ 和 $p(y)$ 分别是 $X$ 和 $Y$ 的边际概率分布

在物理意义上，这个公式表示通过知道变量 $X$ 的值，我们可以减少对变量 $Y$ 不确定性的程度。相互信息越高，说明特征与目标变量之间的关系越强。

使用特征选择方法的步骤：

- 连续概率分箱
  - 同上
- 计算特征和目标之间的相互信息
- 根据相互信息值进行特征选择
  - 对相互信息值进行排序，选择前 k 个特征
  - 根据设定的阈值选择特征

## 包装法 (Wrapper Methods)

常用的包装方法有

- 前向选择 (Forward Selection)
- 后向消除 (Backward Elimination)
- 递归特征消除 (Recursive Feature Elimination, RFE)

- 优点
  - 基于模型的性能进行特征选择，通常能获得更好的结果
  - 模型无关。方法只与模型的输出结果有关，可以应用于各种模型
- 缺点
  - 高算力需求
    - 需要多次训练模型，计算成本较高
  - 过拟合风险
    - 可能会选择在训练集上表现良好的特征，但在测试集上表现不佳

### 前向选择 (Forward Selection)

前向选择是一种递归特征选择方法，从一个空特征集开始，逐步添加特征，直到达到预定的停止条件。具体步骤如下：

- 初始化
  - 初始化一个空的集合 $S$ 来存储已选择的特征，一个未选择的特征集合 $F$ 包含所有特征，一个评估指标
- 在特征中迭代
  - 对于每个未选择的特征 $f$ 在集合 $F$ 中
    - 将特征 $f$ 添加到当前选择的特征集合 $S$ 中，形成新的特征集合 $S' = S \cup \{f\}$
    - 使用特征集合 $S'$ 训练模型，并评估模型性能，记录评估结果
- 选择最优特征
  - 根据训练结果，选择最佳的特征 $f^*$，即使得模型性能最优的特征
- 更新集合
  - 把最优特征添加到已选择的特征集合 $S$ 中，并从未选择的特征集合 $F$ 中移除
- 重复直到结束条件

  - 如特征集合达到预定大小，或模型性能不再显著提升（提升值小于某个阈值）

- 优点
  - 模型需要反复训练，计算量仍然比其他的方法小
- 缺点
  - 和贪心算法类似，忽略了特征之间的相关性，可能得到的特征集合不是最优的。

### 后向消除 (Backward Elimination)

后向消除是一种递归特征选择方法，从包含所有特征的集合开始，逐步移除不重要的特征，直到达到预定的停止条件。具体步骤如下：

- 初始化
  - 初始化一个选择的特征集合 $S$ 包含所有特征，一个评估指标
- 训练模型
  - 使用当前选定的特征集合来训练模型并获得评估结果
- 评估特征重要性
  - 使用特征集删去每个特征 $f$，形成新的特征集合 $S' = S \setminus \{f\}$，并使用 $S'$ 训练模型，评估模型性能，记录评估结果
- 更新选择特征集合
  - 找到移除后使模型性能提升或下降最小的特征 $f^*$，将其从选择的特征集合 $S$ 中移除
- 重复直到结束条件

  - 如特征集合达到预定大小，或模型性能不再显著下降

- 优点
  - 更好的找到了效率和效果之间的平衡，适用于高维数据
- 缺点
  - 依赖模型能力，模型必须从给定数据中全面且正确的学习，否则可能会移除重要特征

### 递归特征消除 (Recursive Feature Elimination, RFE)

递归特征消除 (RFE) 是一种基于模型的特征选择方法，通过递归地训练模型并移除最不重要的特征，直到达到预定的特征数量。具体步骤如下：

- 初始化
  - 选择一个基模型（如线性回归、决策树等），设定目标特征数量 $k$
  - 初始化特征集合 $S$ 包含所有特征
  - 一个评估指标
- 训练基模型
  - 使用当前选择的特征集合 $S$ 训练基模型
- 评估特征重要性
  - 根据基模型的特征重要性评分，识别出最不重要的
- 更新特征集合
  - 移除最不重要的特征 $f^*$，更新特征集合 $S = S \setminus \{f^*\}$
- 重复直到达到终止条件

  - 直到特征集合 $S$ 的大小等于目标特征数量 $k$
  - 所有特征都被迭代过了

- 优点
  - 高效，只需要一个小型的基础模型，不用反复训练复杂模型。
  - 能够模型无关地捕获相关性
- 缺点
  - 依赖模型的可解释性，基础模型需要是可解释的，以便评估特征重要性

## 嵌入法 (Embedded Methods)

嵌入法是一种在模型训练过程中进行特征选择的方法。它将特征选择过程嵌入到模型训练过程中，通过正则化等技术来选择重要的特征。

常用的嵌入法有

- 线性回归：正则化方法
- 树模型：决策树特征重要性

因为嵌入法将特征选择过程与模型训练过程结合在一起，因此它通常比过滤法和包装法更高效。嵌入法能够自动选择重要的特征，同时考虑特征之间的相互关系，从而提高模型的性能和泛化能力。如

- 深度学习：注意力机制

- 优点
  - 高效率
    - 模型只用训练一次，因为选择过程和训练过程结合在一起
  - 更好的模型适应性
    - 特征选择依赖于特定算法，不同模型可能会有不同的特征子集
- 缺点
  - 缺少可解释性
    - 复杂模型的特征重要性结果难以解释
  - 难以检测错误
    - 特征的重要性并没有被显式地给出，只能通过模型表现判断特征是否被正确的使用。

### 线性回归：正则化方法

给定两个集合 $X_i = {x_1, x_2, \ldots, x_n}$ 和 $Y_i = {y_1, y_2, \ldots, y_n}$，线性回归的目标是找到一组参数，使得方程 $\hat{y}_k = \beta_0 \sum_{i=1}^n \beta_i x_i + \varepsilon$ 能够最好地拟合数据，即 $\min_\beta \frac{1}{2n} \sum_{k=1}^n (\hat{y}_k - y_k)^2$。

L1 正则化指的是在正则化中添加一个附加项，也就是 $\min_\beta \left[ \frac{1}{2n}\sum_{k=1}^n (\hat{y}_k - y_k)^2 + \lambda \sum_{j=1}^p |\beta_j| \right]$，其中 $\lambda$ 是正则化参数，在取 $0$ 时表示没有正则化。接下来的步骤是

- 将所有样本标准化，确定值范围的一致性
- 确定一个合适的正则化参数 $\lambda$，通过消融实验进行调整
- 使用新的优化目标来训练模型，系数不为 $0$ 的特征即为被选择的特征

正则化分析能实现特征选择的原因是，当院士目标的轮廓线和正则化项的轮廓线相切时，最优解往往出现在坐标轴上，从而导致某些系数变为零。

![1761039292210](lecture9.assets/1761039292210.png)

- 优点
  - 自动特征选择，不需要额外的特征选择步骤
  - 高计算效率，适用于高维数据
  - 提供了系数的可解释性
- 缺点
  - 对于高度相关的特征，可能只能随机选择一个
  - 需要对数据归一化
  - 只适用于线性模型

### 树模型：决策树特征重要性

决策树指的是一种基于树形结构进行决策的模型。它通过对数据进行递归划分，形成一个树状结构，每个节点表示一个特征，每个分支表示一个特征值的取值范围，叶子节点表示最终的预测结果。

1. 选择一个特征来分割数据
   有多种方法，比如 ID3，CART，MSE
2. 在满足预设条件时终止
   典型的条件是样本数量小于阈值，所有样本属于同一类别或者树达到了最大深度
3. （可选）剪枝
   通过移除一些不必要的节点来简化树结构，防止过拟合

特征重要性是指在决策树模型中，每个特征对模型预测结果的贡献程度。常用的计算方法有基尼重要性 (Gini Importance) 和信息增益 (Information Gain)。这里使用基尼不纯度 (Gini Impurity) 来计算特征重要性

$$
\mathrm{Gini}(D) = 1 - \sum_{i=1}^P p_i^2
$$

其中，$D$ 是数据集，$P$ 是类别数，$p_i$ 是类别 $i$ 在数据集 $D$ 中的比例。此时，特征重要性被定义为

$$
F = \frac{\sum_D \Delta_F \mathrm{Gini}(D)}{
  \sum_F \sum_D \Delta_F \mathrm{Gini}(D)
}
$$

其中，$\Delta_F \mathrm{Gini}(D)$ 是在节点 $D$ 上使用特征 $F$ 进行划分所带来的基尼不纯度的减少。如果一个特征没有被用于任何划分，则其重要性为 $0$。

此时，通过特征的重要性值来选择特征，通常选择重要性值较高的特征。或者选择前 k 个重要性值最高的特征。

- 优点
  - 自动捕捉非线性关系
  - 易于计算
- 缺点
  - 对相关性敏感
  - 倾向于高基数特征

## 降维 (Dimension Reduction)

降维指的是通过数学或者统计的方法把一个高维度的数据转换成低维度的表示方法。

<!-- TODO @ 109/176 @ 4._zh -->